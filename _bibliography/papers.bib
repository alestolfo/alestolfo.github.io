---
---

@article{stolfo2022causal,
  abbr={arXiv},
  title="A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
  author={Stolfo*, A. and Jin*, Z. and Shridhar, K. and Sch{\"o}lkopf, B. and Sachan, M.},
  journal={arXiv preprint},
  year={2022},
  selected={true},
  html={https://arxiv.org/abs/2210.12023}
}

@article{shridhar2022distilling,
  abbr={arXiv},
  title={Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions},
  author={Shridhar*, K. and Stolfo*, A. and Sachan, M.},
  journal={arXiv preprint},
  html={https://arxiv.org/abs/2212.00193},
  selected={true},
  year={2022}
}

@article{shridhar2022longtonotes,
  abbr={arXiv},
  title="Longtonotes: OntoNotes with Longer Coreference Chains",
  author={Shridhar, K. and Monath, N. and Thirukovalluru, R. and Stolfo, A. and Zaheer, M. and McCallum, A. and Sachan, M.},
  journal={arXiv preprint},
  year={2022},
  selected={true},
  html={https://arxiv.org/abs/2210.03650}
}

@inproceedings{stolfo-etal-2022-simple,
    abbr={*SEM},
    title = "A Simple Unsupervised Approach for Coreference Resolution using Rule-based Weak Supervision",
    author = "Stolfo, A.  and
      Tanner, C.  and
      Gupta, V.  and
      Sachan, M.",
    booktitle = "Proceedings of the 11th Joint Conference on Lexical and Computational Semantics,",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.starsem-1.7",
    doi = "10.18653/v1/2022.starsem-1.7",
    pages = "79--88",
    abstract = "Labeled data for the task of Coreference Resolution is a scarce resource, requiring significant human effort. While state-of-the-art coreference models rely on such data, we propose an approach that leverages an end-to-end neural model in settings where labeled data is unavailable. Specifically, using weak supervision, we transfer the linguistic knowledge encoded by Stanford?s rule-based coreference system to the end-to-end model, which jointly learns rich, contextualized span representations and coreference chains. Our experiments on the English OntoNotes corpus demonstrate that our approach effectively benefits from the noisy coreference supervision, producing an improvement over Stanford?s rule-based system (+3.7 F1) and outperforming the previous best unsupervised model (+0.9 F1). Additionally, we validate the efficacy of our method on two other datasets: PreCo and Litbank (+2.5 and +5 F1 on Stanford{'}s system, respectively).",
    selected={true},
    url={https://aclanthology.org/2022.starsem-1.7/},
    html={https://aclanthology.org/2022.starsem-1.7/},
}

